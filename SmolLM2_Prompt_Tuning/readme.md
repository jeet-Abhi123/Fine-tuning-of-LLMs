# Prompt Tuning 

We can save a great deal of money and training hours by using the fantastic Prompt Tuning technique.  In a matter of minutes, I have trained SmolLM2 in the notebook. <br> 
The notebook can also be used with other models if you would like to experiment with different combinations and models.  All you have to do is check out the other models' configurations.
The model, the number of virtual tokens, and the number of training epochs can all be altered. <br> 

Every time we train the refined models, their responses might change.  

![Screenshot 2025-04-14 164816](https://github.com/user-attachments/assets/497e1244-a111-4928-81b7-d7d59c7216f3)

A sequence of additional task specific tunable tokens prepended to the input text.
