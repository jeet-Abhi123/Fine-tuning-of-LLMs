{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Peft Library"
      ],
      "metadata": {
        "id": "7Yfmn2zQEN14"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu_9nBkURirK"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "qHbuTmCkTT19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model and the tokenizers."
      ],
      "metadata": {
        "id": "2sUEsoKnEVOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"HuggingFaceTB/SmolLM2-360M\"\n",
        "NUM_VIRTUAL_TOKENS = 35  # These no. of tokens will be added in the pretrained model, which will be trainable\n",
        "\n",
        "NUM_EPOCHS_PROMPT = 10\n",
        "NUM_EPOCHS_CLASSIFIER = 10\n",
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "0JQaFVtQTT8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Add this line to set the padding token\n",
        "\n",
        "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    device_map = device\n",
        ")"
      ],
      "metadata": {
        "id": "VwKSb8yQTuiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference with the pre trained Smollm2 model"
      ],
      "metadata": {
        "id": "tQh4Pe9wT0lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100):\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        #temperature=0.2,\n",
        "        #top_p=0.95,\n",
        "        #do_sample=True,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=True, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "PGFbt7_bTwkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will run the same sentence on both models in order to compare the pre-trained model with the same model following the prompt-tuning procedure."
      ],
      "metadata": {
        "id": "YDCoXD4tE6wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datasets import load_dataset\n",
        "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ASpFc-8aUBMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_classifier = tokenizer(\"Sentence : Because everyone knows this islam men is the devil. Label : \", return_tensors=\"pt\")\n",
        "foundational_outputs_prompt = get_outputs(foundational_model,\n",
        "                                          input_classifier.to(device),\n",
        "                                          max_new_tokens=3)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "euORMSDqUbFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is generating ambiguous response and it completes the sentence as best as it can."
      ],
      "metadata": {
        "id": "zhvhOdWJUyGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_classifier = \"SetFit/ethos_binary\"\n",
        "\n",
        "def concatenate_columns_classifier(dataset):\n",
        "    def concatenate(example):\n",
        "        example['text'] = \"Sentence : {} Label : {}\".format(example['text'], example['label_text'])\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(concatenate)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "bq9653XpUqB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_classifier = load_dataset(dataset_classifier)\n",
        "data_classifier['train'] = concatenate_columns_classifier(\n",
        "    data_classifier['train'])\n",
        "\n",
        "data_classifier = data_classifier.map(\n",
        "    lambda samples: tokenizer(samples[\"text\"]),\n",
        "    batched=True)\n",
        "train_sample_classifier = data_classifier[\"train\"].remove_columns(\n",
        "    ['label', 'label_text', 'text'])"
      ],
      "metadata": {
        "id": "F0KQnOOLVY3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_classifier"
      ],
      "metadata": {
        "id": "laudbf78Vmlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample_classifier"
      ],
      "metadata": {
        "id": "pF3hFuiqVxH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_sample_classifier[2:3])"
      ],
      "metadata": {
        "id": "e3AK1SQkV9Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt-Tuning Configuration"
      ],
      "metadata": {
        "id": "dxtbQUY5WYhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config_classifier = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,   #This type indicates the model will generate text.\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
        "    prompt_tuning_init_text=\"Indicate if the text contains hate speech or no hate speech.\",\n",
        "    #Number of virtual tokens to be added and trained.\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS,   # 20 virtual tokens\n",
        "    #The pre-trained model.\n",
        "    tokenizer_name_or_path=model_name\n",
        ")"
      ],
      "metadata": {
        "id": "Kd_FJkRtWTwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_classifier = get_peft_model(\n",
        "    foundational_model,\n",
        "    generation_config_classifier)\n",
        "print(peft_model_classifier.print_trainable_parameters())  # Get the total percentage of trainable parameters"
      ],
      "metadata": {
        "id": "OvojA_f3WT2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Did you notice the decrease in trainable parameters?** That's incredible. <br>**0.0093% of the available paramaters will be trained.**"
      ],
      "metadata": {
        "id": "USIcpgCkW_1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "working_dir = \"/content/Prompt_Tuning_SmolLM2/\"\n",
        "\n",
        "#Is best to store the models in separate folders.\n",
        "#Create the name of the directories where to store the models.\n",
        "output_directory_classifier =  os.path.join(working_dir, \"peft_outputs_classifier\")\n",
        "\n",
        "#Just creating the directoris if not exist.\n",
        "if not os.path.exists(working_dir):\n",
        "    os.mkdir(working_dir)"
      ],
      "metadata": {
        "id": "NephWjSnW3Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "def create_training_arguments(path, learning_rate=0.0035, epochs=6, autobatch=True):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
        "        #use_cpu=True, # This is necessary for CPU clusters.\n",
        "        auto_find_batch_size=autobatch, # Find a suitable batch size that will fit into memory automatically\n",
        "        learning_rate= learning_rate, # Higher learning rate than full fine-tuning\n",
        "        #per_device_train_batch_size=4,\n",
        "        num_train_epochs=epochs,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    return training_args"
      ],
      "metadata": {
        "id": "EHxMy3C-X-Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args_classifier = create_training_arguments(\n",
        "    output_directory_classifier,\n",
        "    3e-2,\n",
        "    NUM_EPOCHS_CLASSIFIER)"
      ],
      "metadata": {
        "id": "jGgUo3reXrzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "4llYHoyWYKaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "def create_trainer(model, training_args, train_dataset):\n",
        "    trainer = Trainer(\n",
        "        model=model, # We pass in the PEFT version of the foundation model, SmolLM2\n",
        "        args=training_args, #arguments\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
        "    )\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "8kOsJQOqYawe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_classifier = create_trainer(peft_model_classifier,\n",
        "                                   training_args_classifier,\n",
        "                                   train_sample_classifier)\n",
        "trainer_classifier.train()"
      ],
      "metadata": {
        "id": "UM8ynDC1Xy0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Model"
      ],
      "metadata": {
        "id": "l3eGBTfNFMbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_classifier.model.save_pretrained(output_directory_classifier)"
      ],
      "metadata": {
        "id": "4uz0TrXiYt9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "loaded_model_peft = PeftModel.from_pretrained(foundational_model,\n",
        "                                         output_directory_classifier,\n",
        "                                         #device_map=device,\n",
        "                                         is_trainable=False)"
      ],
      "metadata": {
        "id": "Q36sbch7ec5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_peft.load_adapter(output_directory_classifier, adapter_name=\"classifier\")\n",
        "loaded_model_peft.set_adapter(\"classifier\")"
      ],
      "metadata": {
        "id": "lA1ejoOdeBr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_classifier = tokenizer(\"Sentence : Because everyone knows this islam men is the devil.  Label : \", return_tensors=\"pt\").to(device)\n",
        "\n",
        "loaded_model_sentences_outputs = get_outputs(loaded_model_peft,\n",
        "                                             input_classifier,\n",
        "                                             max_new_tokens=3)\n",
        "print(tokenizer.batch_decode(loaded_model_sentences_outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "bdXu_sjLeUQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z02oYdsXgQwW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}