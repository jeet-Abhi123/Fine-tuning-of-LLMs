{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing necesssary libraries"
      ],
      "metadata": {
        "id": "2e7xVEibKqbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "HVqTczl4W8UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ICs0c1WBZjD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1k8LPG2We8o"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import GenerationConfig, TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# health-Q&A dataset\n",
        "huggingface_dataset_name = \"RafaelMPereira/HealthCareMagic-100k-Chat-Format-en\"\n",
        "\n",
        "ds = load_dataset(huggingface_dataset_name, split='train')\n"
      ],
      "metadata": {
        "id": "zBJlQwccW5xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Function to split text into \"human\" and \"bot\" columns\n",
        "def split_text(example):\n",
        "    match = re.match(r\"<human>:\\s*(.*?)\\s*<bot>:\\s*(.*)\", example[\"text\"], re.DOTALL)\n",
        "    if match:\n",
        "        return {\"human\": match.group(1).strip(), \"bot\": match.group(2).strip()}\n",
        "    return {\"human\": \"\", \"bot\": \"\"}  # Handle cases where the format is incorrect\n",
        "\n",
        "# Apply the function to the dataset\n",
        "ds = ds.map(split_text, remove_columns=[\"text\"])\n",
        "ds"
      ],
      "metadata": {
        "id": "litEQgfEXjj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting only 12k samples for faster fine-tuning\n",
        "ds = ds.select(range(12000))\n",
        "ds"
      ],
      "metadata": {
        "id": "kxNilfxNXuvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test\n",
        "train_test_split = ds.train_test_split(test_size=0.05, seed=42)\n",
        "train_dataset = train_test_split['train']\n",
        "test_dataset = train_test_split['test']\n",
        "\n",
        "train_validation_split = train_dataset.train_test_split(test_size=0.05, seed=42)\n",
        "train_dataset = train_validation_split['train']\n",
        "validation_dataset = train_validation_split['test']\n",
        "\n",
        "print(ds)"
      ],
      "metadata": {
        "id": "duYRqNmYX1Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
      ],
      "metadata": {
        "id": "71Cv8fznYD8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"My Device: {}\".format(my_device))"
      ],
      "metadata": {
        "id": "TF4LP3ugYMrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model\n",
        "model_name = \"HuggingFaceTB/SmolLM2-360M\" # HuggingFaceTB/SmolLM2-135M, HuggingFaceTB/SmolLM2-360M, HuggingFaceTB/SmolLM2-1.7B, HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(my_device)"
      ],
      "metadata": {
        "id": "lEPutR_8YOk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token  # decoder-only models typically don't have a padding token pre-defined\n",
        "model.config.pad_token_id = tokenizer.pad_token_id # padding is needed for batching"
      ],
      "metadata": {
        "id": "DkH2BzEXYRHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat template\n",
        "def generate_chat_template(query_text, my_tokenizer, my_device, instruct_model=False):\n",
        "    # create a system message\n",
        "\n",
        "    if instruct_model:\n",
        "        messages = [{\"role\": \"user\", \"content\": query_text}]\n",
        "        input_text = my_tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        inputs = my_tokenizer.encode(input_text, return_tensors=\"pt\").to(my_device)\n",
        "    else:\n",
        "        inputs = my_tokenizer.encode(query_text, return_tensors=\"pt\").to(my_device)\n",
        "\n",
        "    return inputs\n",
        "\n",
        "def generate_output(my_inputs, my_tokenizer, my_model, max_tokens = 50, temp = 0.3, top_p = 0.9, top_k=50, penalty_score=1.2, do_sample = True, instruct_model=False):\n",
        "\n",
        "    if instruct_model:\n",
        "        outputs = my_model.generate(my_inputs, max_new_tokens=max_tokens, temperature=temp, top_p=top_p, top_k=top_k, repetition_penalty=penalty_score, do_sample=do_sample)\n",
        "        output_text = my_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        cleaned_output_text = output_text.split(\"<|im_start|>assistant\")[1].split(\"<|im_end|>\")[0].strip()\n",
        "    else:\n",
        "         outputs = my_model.generate(my_inputs, max_new_tokens=max_tokens, temperature=temp, top_p=top_p, top_k=top_k, repetition_penalty=penalty_score, do_sample=do_sample,\n",
        "                                     eos_token_id=my_tokenizer.eos_token_id)\n",
        "         cleaned_output_text = my_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return cleaned_output_text"
      ],
      "metadata": {
        "id": "LwTCD6K1YUII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_generate_chat_template(query_text, my_tokenizer, my_device, instruct_model=False):\n",
        "    \"\"\"\n",
        "    Formats the query based on the instruction tuning prompt template.\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply the updated instruction-tuned prompt template\n",
        "    formatted_prompt = f\"\"\"\n",
        "    ### Instruction:\n",
        "    You are an AI medical assistant. Respond to the patient request, help them in curing the problem which they are facing by providing them proper diagnosis.\n",
        "\n",
        "    ### Patient Request:\n",
        "    {query_text}\n",
        "\n",
        "    ### Response:\n",
        "    \"\"\"\n",
        "\n",
        "    if instruct_model:\n",
        "        messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
        "        input_text = my_tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        inputs = my_tokenizer.encode(input_text, return_tensors=\"pt\").to(my_device)\n",
        "    else:\n",
        "        inputs = my_tokenizer.encode(formatted_prompt, return_tensors=\"pt\").to(my_device)\n",
        "\n",
        "    return inputs\n",
        "\n",
        "def tune_generate_output(my_inputs, my_tokenizer, my_model, max_tokens=50, temp=0.3, top_p=0.9, top_k=50, penalty_score=1.2, do_sample=True, instruct_model=False):\n",
        "    \"\"\"\n",
        "    Generates a response from the model based on the input.\n",
        "    \"\"\"\n",
        "\n",
        "    outputs = my_model.generate(\n",
        "        my_inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temp,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=penalty_score,\n",
        "        do_sample=do_sample,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=my_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode output and clean it up\n",
        "    output_text = my_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Ensure safe parsing without hardcoded token removal\n",
        "    cleaned_output_text = output_text.strip()\n",
        "\n",
        "    return cleaned_output_text"
      ],
      "metadata": {
        "id": "ZIFdXng_Yk6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_query = \"Question: What is the capital of Germany?\"\n",
        "sample_inputs = generate_chat_template(sample_query, tokenizer, my_device)\n",
        "sample_output = generate_output(sample_inputs, tokenizer, model, max_tokens=20, temp = 0.6, top_p = 0.6, top_k=50, penalty_score=1.2, do_sample = True, instruct_model=False)\n",
        "\n",
        "print(sample_query)\n",
        "print(\"=\"*10)\n",
        "print(sample_output)"
      ],
      "metadata": {
        "id": "COWkIJGJYk8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "LUEkFRJeZzeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Inference on the pretrained model\n"
      ],
      "metadata": {
        "id": "Rlmxq23fMwyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 10\n",
        "\n",
        "sample_query = train_dataset['human'][sample_idx]\n",
        "sample_response = train_dataset['bot'][sample_idx]\n",
        "sample_inputs = generate_chat_template(sample_query, tokenizer, my_device)\n",
        "sample_output = generate_output(sample_inputs, tokenizer, model, max_tokens=50, temp = 0.7, top_p = 0.6, top_k=50, penalty_score=1.2, do_sample = True, instruct_model=False)\n",
        "\n",
        "print(\"=\"*10)\n",
        "print(sample_query)\n",
        "print(\"=\"*10)\n",
        "print(sample_response)\n",
        "print(\"=\"*10)\n",
        "print(sample_output)"
      ],
      "metadata": {
        "id": "D4QlGSaHZdD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(print_number_of_trainable_model_parameters(model))"
      ],
      "metadata": {
        "id": "Rjm0-pfiaUY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model  # this will show us the model architecture"
      ],
      "metadata": {
        "id": "rgAnr8R5aUcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LORA ##\n",
        "lora_config = LoraConfig(\n",
        "    r=8, # Rank\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], #\"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, lora_config).to(my_device)\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "id": "PeoDB-6-aUgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Just training 0.45% parameters of the total model**"
      ],
      "metadata": {
        "id": "FH0uUS4EM_GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TRY DIFFERENT PROMPTS ##\n",
        "sample_idx = 0\n",
        "\n",
        "sample_query = train_dataset['human'][sample_idx]\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "### Instruction:\n",
        "You are an AI medical assistant. Respond to the patient request, help them in curing the problem which they are facing by providing them proper diagnosis.\n",
        "\n",
        "### Patient Request:\n",
        "{sample_query}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(my_device)\n",
        "outputs = model.generate(inputs, max_new_tokens=50, temperature=0.6, top_p=0.7, top_k=50, repetition_penalty=1.2, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n",
        "sample_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "sample_response = train_dataset['bot'][sample_idx]\n",
        "\n",
        "print(\"=\"*10)\n",
        "print(sample_query)\n",
        "print(\"=\"*10)\n",
        "print(sample_response)\n",
        "print(\"=\"*10)\n",
        "print(sample_output)"
      ],
      "metadata": {
        "id": "xUvmJRiAaknp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 0\n",
        "\n",
        "sample_query = train_dataset['human'][sample_idx]\n",
        "sample_response = train_dataset['bot'][sample_idx]\n",
        "\n",
        "sample_inputs = tune_generate_chat_template(sample_query, tokenizer, my_device)\n",
        "sample_output = tune_generate_output(sample_inputs, tokenizer, model, max_tokens=100, temp = 0.7, top_p = 0.6, top_k=50, penalty_score=1.2, do_sample = True, instruct_model=False)\n",
        "\n",
        "\n",
        "print(\"=\"*10)\n",
        "print(sample_query)\n",
        "print(\"=\"*10)\n",
        "print(sample_response)\n",
        "print(\"=\"*10)\n",
        "print(sample_output)"
      ],
      "metadata": {
        "id": "wX_74pzvbTke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes the dataset using the fine-tuning instruction prompt format.\n",
        "    Handles batched processing by iterating over each example.\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct the full prompt for each example in the batch\n",
        "    formatted_prompts = [\n",
        "        f\"\"\"\n",
        "        ### Instruction:\n",
        "        You are an AI medical assistant. Respond to the patient request, help them in curing the problem which they are facing by providing them proper diagnosis.\n",
        "\n",
        "        ### Patient Request:\n",
        "        {human}\n",
        "\n",
        "        ### Response:\n",
        "        {bot}\n",
        "        \"\"\"\n",
        "        for human, bot in zip(examples[\"human\"], examples[\"bot\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenize each formatted prompt\n",
        "    tokenized_outputs = tokenizer(\n",
        "        formatted_prompts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,  # Adjust as needed\n",
        "    )\n",
        "    # Return tokenized inputs\n",
        "    return {\n",
        "        \"input_ids\": tokenized_outputs[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized_outputs[\"attention_mask\"],\n",
        "        \"labels\": tokenized_outputs[\"input_ids\"],  # Causal LM: labels = input_ids\n",
        "    }\n",
        "\n",
        "# Tokenizing and processing all dataset splits\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['human', 'bot'])\n",
        "\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns(['human', 'bot'])\n",
        "\n",
        "tokenized_validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_validation_dataset = tokenized_validation_dataset.remove_columns(['human', 'bot'])"
      ],
      "metadata": {
        "id": "QRyZ-eGEb4nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## now merge the splits into a single dataset format\n",
        "tokenized_datasets = DatasetDict({\n",
        "    'train': tokenized_train_dataset,\n",
        "    'validation': tokenized_validation_dataset,\n",
        "    'test': tokenized_test_dataset\n",
        "})\n",
        "\n",
        "print(f\"Shapes of the datasets:\")\n",
        "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
        "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
        "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
        "\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "GCHBMNSpc3BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "ylHDsjpeNhH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### PEFT Training ####\n",
        "output_dir = f'/content/SmolLM2-fine-tuned/health_qa_base_tune_peft-350M-{str(time.strftime(\"%Y_%m_%d_%H_%M\"))}'   # define the directory to store the results\n",
        "\n",
        "## DO IT FALSE, IF THERE ARE ANY VALUABLE MODELS!!!!\n",
        "if False:\n",
        "  shutil.rmtree('tuned_models_SmolLM2')\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "step_size = 100 # we have 11k in total\n",
        "lr = 1e-3\n",
        "n_epochs = 2\n",
        "batch_size = 8\n",
        "gradient_acc = 4\n",
        "warmup_steps = 50 # previously:0\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=False,\n",
        "    learning_rate=lr,\n",
        "    num_train_epochs=n_epochs,\n",
        "    logging_steps=step_size,  # Logs training loss every x.th steps\n",
        "    save_steps=step_size,\n",
        "    warmup_steps=warmup_steps,\n",
        "    logging_strategy=\"steps\",  # Ensures logs are printed every x.th steps\n",
        "    evaluation_strategy=\"steps\",  # Runs validation after each epoch\n",
        "    save_strategy=\"steps\",  # Saves model checkpoints after each epoch\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=gradient_acc,\n",
        "    no_cuda=not torch.cuda.is_available(),\n",
        "    report_to=\"none\"  # Disable WandB\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "   eval_dataset=tokenized_datasets['validation']\n",
        ")\n",
        "\n",
        "peft_trainer.train()"
      ],
      "metadata": {
        "id": "J7MHWrH-e7Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "\n",
        "# Define the model path (same as output_dir)\n",
        "model_path = \"/content/SmolLM2-fine-tuned/health_qa_base_tune_peft-350M-2025_04_07_12_50/checkpoint-676/\"  # Replace with the actual timestamp\n",
        "\n",
        "# Load base model (Make sure it's the same base model used for fine-tuning)\n",
        "base_model_name = \"HuggingFaceTB/SmolLM2-360M\"  # Replace with the actual base model name\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
        "\n",
        "# Load fine-tuned LoRA model\n",
        "peft_model = PeftModel.from_pretrained(base_model, model_path)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "peft_model.to(device)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "peft_model.eval()\n"
      ],
      "metadata": {
        "id": "mnGxLWm_m3Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on fine-tuned model\n",
        "def generate_response(prompt, max_length=256):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        output = peft_model.generate(**inputs, max_length=max_length)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"What are the symptoms of depression?\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "kt7Hz4_Qm3EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Merge LoRA weights into base model\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "# Save & push merged full model\n",
        "merged_model.push_to_hub(\"Kaith-jeet123/SmolLM2-fine-tuned-healthQA\")\n",
        "tokenizer.push_to_hub(\"Kaith-jeet123/SmolLM2-fine-tuned-healthQA\")\n"
      ],
      "metadata": {
        "id": "ybh73bAJm3G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QIEajZFD-8n3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}